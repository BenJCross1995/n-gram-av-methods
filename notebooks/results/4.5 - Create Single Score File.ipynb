{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9aff6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union, Sequence, Optional\n",
    "from from_root import from_root\n",
    "\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from read_and_write_docs import read_excel_sheets, read_rds\n",
    "from utils import list_xlsx_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c921685",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt2\"]\n",
    "\n",
    "corpora = [\n",
    "    \"Wiki\", \"Enron\", \"Perverted Justice\", \"StackExchange\", \"ACL\",\n",
    "    \"TripAdvisor\", \"The Apricity\", \"Koppel's Blogs\", \"The Telegraph\",\n",
    "    \"Reddit\"\n",
    "]\n",
    "\n",
    "data_types = [\"training\", \"test\"]\n",
    "\n",
    "raw_subdirs = (\n",
    "    \"raw\", \"raw_100\", \"raw_200\", \"raw_300\", \"raw_400\", \"raw_500\",\n",
    "    \"raw_600\", \"raw_700\", \"raw_800\", \"raw_900\", \"raw_1000\"\n",
    ")\n",
    "\n",
    "base_loc = \"/Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs\"\n",
    "\n",
    "metadata_base_loc = \"/Volumes/BCross/datasets/author_verification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb95d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_complete_to_metadata(metadata_base_loc, data_type, corpus, model, excel_files, max_context_tokens=None):\n",
    "    \n",
    "    metadata_loc = f\"{metadata_base_loc}/{data_type}/doc_level_metadata.rds\"\n",
    "    \n",
    "    metadata = read_rds(metadata_loc)\n",
    "    metadata = metadata[metadata['corpus'] == corpus]\n",
    "    metadata['scoring_model'] = model\n",
    "    metadata['max_context_tokens'] = max_context_tokens\n",
    "    \n",
    "    file_names = [ef.name for ef in excel_files]\n",
    "    # df with filename + completed=True\n",
    "    df = pd.DataFrame({\n",
    "        \"filename\": file_names,\n",
    "        \"completed\": True\n",
    "    })\n",
    "\n",
    "    # left join onto metadata_df and fill missing completed with False\n",
    "    metadata_df = (\n",
    "        metadata\n",
    "        .merge(df, on=\"filename\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    metadata_df[\"completed\"] = metadata_df[\"completed\"].fillna(False).astype(bool)\n",
    "    metadata_df[\"scored\"] = False\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3317d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_problem_complete_metadata(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups by (data_type, corpus, scoring_model, problem) and returns:\n",
    "      - num_files: total rows\n",
    "      - files_completed: count where completed == True\n",
    "      - files_scored: count where scored == True\n",
    "      - problem_completed: True if num_files == files_scored\n",
    "    \"\"\"\n",
    "    group_cols = [\"data_type\", \"corpus\", \"scoring_model\", \"max_context_tokens\", \"problem\"]\n",
    "\n",
    "    out = (\n",
    "        metadata\n",
    "        .groupby(group_cols, dropna=False)\n",
    "        .agg(\n",
    "            num_files=(\"filename\", \"size\"),\n",
    "            files_completed=(\"completed\", lambda s: int(s.fillna(False).astype(bool).sum())),\n",
    "            files_scored=(\"scored\", lambda s: int(s.fillna(False).astype(bool).sum())),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    out[\"problem_completed\"] = out[\"num_files\"] == out[\"files_scored\"]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7740e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_max_context_tokens(raw_dir_name: Optional[str]) -> Union[int, str, None]:\n",
    "    \"\"\"\n",
    "    raw_dir_name: \"raw\" or \"raw_200\" / \"raw_300\" etc (or None).\n",
    "    Returns:\n",
    "      - \"None\" (string) if raw_dir_name == \"raw\"\n",
    "      - int if suffix exists (e.g. \"raw_200\" -> 200)\n",
    "      - None (Python) if raw_dir_name is None or doesn't match expected pattern\n",
    "    \"\"\"\n",
    "    if raw_dir_name is None:\n",
    "        return None\n",
    "\n",
    "    s = str(raw_dir_name).strip()\n",
    "    if s == \"raw\":\n",
    "        return \"None\"\n",
    "\n",
    "    m = re.fullmatch(r\"raw_(\\d+)\", s)\n",
    "    return int(m.group(1)) if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da16350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_for_combine(path: Path, *, engine: str | None = None) -> pd.DataFrame:\n",
    "    # keep_default_na=False stops pandas treating \"None\" as NaN\n",
    "    df = pd.read_excel(path, engine=engine, keep_default_na=False)\n",
    "\n",
    "    # Normalise max_context_length so blanks become the string \"None\"\n",
    "    if \"max_context_length\" in df.columns:\n",
    "        def _norm(v):\n",
    "            # \"\" happens if Excel cell is blank; NaN can still appear in some cases\n",
    "            if v == \"\" or v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "                return \"None\"\n",
    "            # make 200.0 -> 200 (optional nicety)\n",
    "            if isinstance(v, float) and v.is_integer():\n",
    "                return int(v)\n",
    "            return v\n",
    "\n",
    "        df[\"max_context_length\"] = df[\"max_context_length\"].map(_norm)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78949547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_phrase_scores_with_metadata_prefix(\n",
    "    base_loc: str | Path,\n",
    "    data_type: str,\n",
    "    corpus: str,\n",
    "    model: str,\n",
    "    *,\n",
    "    raw_subdirs: Sequence[str] = (\"raw\",),\n",
    "    save_dirname: str | None = None,\n",
    "    meta_sheet: str = \"metadata\",\n",
    "    phrase_sheet: str = \"phrase score\",\n",
    "    output_name: str = \"phrase_scores_with_meta.xlsx\",\n",
    "    recursive: bool = False,\n",
    "    engine: str | None = None,\n",
    "    overwrite: bool = False,\n",
    "    combine_files: bool = False,\n",
    "    combined_file_prefix: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For each .xlsx in {base_loc}/{data_type}/{corpus}/{model}/{raw_subdir}:\n",
    "      - read `meta_sheet` and `phrase_sheet`\n",
    "      - take metadata columns from start up to and including `compute_type`\n",
    "      - take FIRST ROW of that metadata subset\n",
    "      - repeat it to match number of rows in phrase score\n",
    "      - prepend those columns to the phrase score dataframe\n",
    "    Then concat all files in the directory and save to `output_name`.\n",
    "\n",
    "    Optionally: `combine_files=True` will combine the per-raw_subdir outputs into one file.\n",
    "    Assumes you already have:\n",
    "      - list_xlsx_files(dir, recursive=...)\n",
    "      - read_excel_sheets(path, sheet_names=[...]) -> dict[str, pd.DataFrame]\n",
    "      - _read_for_combine(path, engine=...) -> pd.DataFrame  (only needed if combine_files=True)\n",
    "    \"\"\"\n",
    "    base_loc = Path(base_loc)\n",
    "\n",
    "    if isinstance(raw_subdirs, str):\n",
    "        raw_subdirs = (raw_subdirs,)\n",
    "\n",
    "    model_dir = base_loc / data_type / corpus / model\n",
    "    save_base = (model_dir / save_dirname) if save_dirname else model_dir\n",
    "    save_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    disambiguate_names = len(tuple(raw_subdirs)) > 1\n",
    "\n",
    "    def _render_name(name: str, raw_subdir: str, *, disambiguate: bool) -> str:\n",
    "        if \"{raw_subdir}\" in name:\n",
    "            return name.format(raw_subdir=raw_subdir)\n",
    "        if disambiguate:\n",
    "            p = Path(name)\n",
    "            return f\"{p.stem}_{raw_subdir}{p.suffix}\"\n",
    "        return name\n",
    "\n",
    "    def _combined_name(name: str, prefix: str | None) -> str:\n",
    "        prefix = prefix or \"\"\n",
    "        if \"{raw_subdir}\" in name:\n",
    "            return name.format(raw_subdir=\"combined\")\n",
    "        p = Path(name)\n",
    "        return f\"{prefix}{p.stem}_combined{p.suffix}\"\n",
    "\n",
    "    def _prefix_metadata(meta: pd.DataFrame) -> list[str]:\n",
    "        if meta.empty:\n",
    "            return []\n",
    "        if \"compute_type\" in meta.columns:\n",
    "            end = meta.columns.get_loc(\"compute_type\")\n",
    "            return list(meta.columns[: end + 1])  # inclusive\n",
    "        # fallback: if compute_type missing, just use all columns\n",
    "        return list(meta.columns)\n",
    "\n",
    "    def _process_one_file(xlsx_path: Path) -> pd.DataFrame | None:\n",
    "        try:\n",
    "            data = read_excel_sheets(xlsx_path, [meta_sheet, phrase_sheet])\n",
    "        except Exception as e:\n",
    "            print(f\"  WARN: failed reading sheets from {xlsx_path.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if meta_sheet not in data or phrase_sheet not in data:\n",
    "            print(f\"  WARN: missing sheet(s) in {xlsx_path.name} (need '{meta_sheet}' + '{phrase_sheet}')\")\n",
    "            return None\n",
    "\n",
    "        meta = data[meta_sheet]\n",
    "        phrase = data[phrase_sheet]\n",
    "\n",
    "        if meta is None or meta.empty:\n",
    "            print(f\"  WARN: empty '{meta_sheet}' in {xlsx_path.name}\")\n",
    "            return None\n",
    "        if phrase is None or phrase.empty:\n",
    "            # nothing to contribute\n",
    "            return None\n",
    "\n",
    "        prefix_cols = _prefix_metadata(meta)\n",
    "        if not prefix_cols:\n",
    "            return None\n",
    "\n",
    "        if \"compute_type\" not in meta.columns:\n",
    "            print(f\"  WARN: 'compute_type' not found in {xlsx_path.name} metadata; using all metadata columns\")\n",
    "\n",
    "        meta_row = meta.loc[0, prefix_cols]\n",
    "\n",
    "        # repeat first metadata row to match phrase score length\n",
    "        meta_rep = pd.DataFrame([meta_row.to_dict()] * len(phrase))\n",
    "\n",
    "        # avoid duplicate column names: metadata wins\n",
    "        overlap = [c for c in phrase.columns if c in meta_rep.columns]\n",
    "        if overlap:\n",
    "            phrase = phrase.drop(columns=overlap)\n",
    "\n",
    "        out = pd.concat(\n",
    "            [meta_rep.reset_index(drop=True), phrase.reset_index(drop=True)],\n",
    "            axis=1,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "    def _paths_for(raw_subdir: str):\n",
    "        raw_dir = model_dir / raw_subdir\n",
    "        out_name = _render_name(output_name, raw_subdir, disambiguate=disambiguate_names)\n",
    "        out_path = save_base / out_name\n",
    "        return raw_dir, out_path\n",
    "\n",
    "    # -------------------------\n",
    "    # Per-raw_subdir builds\n",
    "    # -------------------------\n",
    "    for raw_subdir in raw_subdirs:\n",
    "        raw_dir, out_path = _paths_for(raw_subdir)\n",
    "\n",
    "        if not overwrite and out_path.exists():\n",
    "            print(f\"SKIP (exists): {out_path}\")\n",
    "            continue\n",
    "\n",
    "        if not raw_dir.exists():\n",
    "            print(f\"SKIP (no dir): {raw_dir}\")\n",
    "            continue\n",
    "\n",
    "        excel_files = list_xlsx_files(raw_dir, recursive=recursive)\n",
    "        if not excel_files:\n",
    "            print(f\"SKIP (no files): {raw_dir}\")\n",
    "            continue\n",
    "\n",
    "        file_dfs: list[pd.DataFrame] = []\n",
    "        for ef in excel_files:\n",
    "            df = _process_one_file(Path(ef))\n",
    "            if df is not None and not df.empty:\n",
    "                file_dfs.append(df)\n",
    "\n",
    "        if not file_dfs:\n",
    "            print(f\"SKIP (no readable data): {raw_dir}\")\n",
    "            continue\n",
    "\n",
    "        results = pd.concat(file_dfs, ignore_index=True)\n",
    "\n",
    "        # deterministic ordering if columns exist (optional but usually helpful)\n",
    "        sort_cols = [c for c in [\"sample_id\", \"min_token_size\", \"phrase_num\", \"phrase_occurrence\"] if c in results.columns]\n",
    "        if sort_cols:\n",
    "            results = results.sort_values(sort_cols, kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "        results.to_excel(out_path, index=False, engine=engine)\n",
    "        print(f\"SAVED: {out_path}  (rows={len(results)})\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Combined outputs across raw_subdirs (optional)\n",
    "    # -------------------------\n",
    "    if not combine_files:\n",
    "        return\n",
    "\n",
    "    combined_results_path = model_dir / _combined_name(output_name, combined_file_prefix)\n",
    "\n",
    "    if not overwrite and combined_results_path.exists():\n",
    "        print(f\"SKIP (combined exists): {combined_results_path}\")\n",
    "        return\n",
    "\n",
    "    combined_parts: list[pd.DataFrame] = []\n",
    "    for raw_subdir in raw_subdirs:\n",
    "        _, out_path = _paths_for(raw_subdir)\n",
    "        if out_path.exists():\n",
    "            df = _read_for_combine(out_path, engine=engine)\n",
    "            if \"raw_subdir\" not in df.columns:\n",
    "                df.insert(len(df.columns), \"raw_subdir\", raw_subdir)\n",
    "            combined_parts.append(df)\n",
    "        else:\n",
    "            print(f\"  WARN: missing per-subdir file (skip in combine): {out_path}\")\n",
    "\n",
    "    if not combined_parts:\n",
    "        print(\"SKIP (combined): no per-subdir result files found to combine\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(combined_parts, ignore_index=True)\n",
    "    sort_cols = [c for c in [\"data_type\", \"corpus\", \"scoring_model\", \"max_context_length\", \"sample_id\", \"min_token_size\"] if c in combined.columns]\n",
    "    if sort_cols:\n",
    "        combined = combined.sort_values(sort_cols, kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    combined.to_excel(combined_results_path, index=False, engine=engine)\n",
    "    print(f\"SAVED (combined): {combined_results_path}  (rows={len(combined)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e971ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Wiki/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Enron/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Perverted Justice/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/StackExchange/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/ACL/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/TripAdvisor/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Apricity/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw.xlsx  (rows=5651)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_100.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_200.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_300.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_400.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_500.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_600.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_700.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_800.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_900.xlsx  (rows=14010)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/The Telegraph/gpt2/raw_results/phrase_scores_raw_1000.xlsx  (rows=13750)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw.xlsx  (rows=1709)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_100.xlsx  (rows=190866)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_200.xlsx  (rows=190866)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_300.xlsx  (rows=190797)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_400.xlsx  (rows=190560)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_500.xlsx  (rows=190227)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_600.xlsx  (rows=189569)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_700.xlsx  (rows=188900)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_800.xlsx  (rows=188220)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_900.xlsx  (rows=187278)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/training/Reddit/gpt2/raw_results/phrase_scores_raw_1000.xlsx  (rows=185728)\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/StackExchange/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/ACL/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_600.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_700.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_800.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_900.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/TripAdvisor/gpt2/raw_results/phrase_scores_raw_1000.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_100.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_200.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_300.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_400.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_500.xlsx\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_600.xlsx  (rows=54681)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_700.xlsx  (rows=54681)\n",
      "  WARN: failed reading sheets from 4429_lumi_subforum_208_dating_and_relationships vs 446_radojica_subforum_315_geopolitics.xlsx: File is not a zip file\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_800.xlsx  (rows=54639)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_900.xlsx  (rows=54681)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Apricity/gpt2/raw_results/phrase_scores_raw_1000.xlsx  (rows=54681)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw.xlsx  (rows=191849)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_100.xlsx  (rows=247778)\n",
      "  WARN: failed reading sheets from author_569_post_2 vs author_569_post_1.xlsx: [Errno 9] Bad file descriptor\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_200.xlsx  (rows=247737)\n",
      "  WARN: failed reading sheets from author_23_post_3 vs author_23_post_1.xlsx: Excel file format cannot be determined, you must specify an engine manually.\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_300.xlsx  (rows=247721)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_400.xlsx  (rows=247778)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_500.xlsx  (rows=247778)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_600.xlsx  (rows=247778)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_700.xlsx  (rows=247725)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_800.xlsx  (rows=247725)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_900.xlsx  (rows=247677)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Koppel's Blogs/gpt2/raw_results/phrase_scores_raw_1000.xlsx  (rows=247550)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw.xlsx  (rows=10318)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_100.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_200.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_300.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_400.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_500.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_600.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_700.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_800.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_900.xlsx  (rows=19901)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/The Telegraph/gpt2/raw_results/phrase_scores_raw_1000.xlsx  (rows=19315)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw.xlsx  (rows=2140)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_100.xlsx  (rows=291108)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_200.xlsx  (rows=291108)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_300.xlsx  (rows=291108)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_400.xlsx  (rows=291108)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_500.xlsx  (rows=291108)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_600.xlsx  (rows=291108)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_700.xlsx  (rows=291108)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_800.xlsx  (rows=291058)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_900.xlsx  (rows=290880)\n",
      "SAVED: /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Reddit/gpt2/raw_results/phrase_scores_raw_1000.xlsx  (rows=290780)\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    for data_type in data_types:\n",
    "        for corpus in corpora:\n",
    "            try:\n",
    "                build_and_save_phrase_scores_with_metadata_prefix(\n",
    "                    base_loc=base_loc,\n",
    "                    data_type=data_type,\n",
    "                    corpus=corpus,\n",
    "                    model=model_name,\n",
    "                    raw_subdirs = raw_subdirs,\n",
    "                    save_dirname = \"raw_results\",\n",
    "                    meta_sheet = \"metadata\",\n",
    "                    phrase_sheet = \"phrase score\",\n",
    "                    output_name = \"phrase_scores.xlsx\",\n",
    "                    recursive = False,\n",
    "                    engine = \"openpyxl\",\n",
    "                    overwrite = False,\n",
    "                    combine_files = False,\n",
    "                    combined_file_prefix = None,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Missing/failed for model={model_name}, data_type={data_type}, corpus={corpus}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
