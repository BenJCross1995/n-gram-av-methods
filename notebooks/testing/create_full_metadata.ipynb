{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357fceeb",
   "metadata": {},
   "source": [
    "# Create Document-Level Metadata\n",
    "\n",
    "Here we create the document level metadata by processing the metadata for all corpuses. This is useful to only include the problems where we have a full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "444a2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from from_root import from_root\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds, write_rds\n",
    "from utils import apply_temp_doc_id, build_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2334168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"/Volumes/BCross/datasets/author_verification\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ada04",
   "metadata": {},
   "source": [
    "## Function to create the metadata\n",
    "\n",
    "Here we create the metadata and save it to create the document level metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8149eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_metadata(\n",
    "    base_loc: str | Path,\n",
    "    split: str,  # \"training\" or \"test\"\n",
    "    *,\n",
    "    verbose: bool = True,\n",
    "    write_file: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    base_loc = Path(base_loc)\n",
    "\n",
    "    if split not in {\"training\", \"test\"}:\n",
    "        raise ValueError(\"split must be 'training' or 'test'\")\n",
    "\n",
    "    metadata_rds_loc = base_loc / split / \"metadata.rds\"\n",
    "    metadata_save_loc = base_loc / split / \"doc_level_metadata.rds\"\n",
    "    \n",
    "    metadata = read_rds(metadata_rds_loc)\n",
    "    if \"corpus\" not in metadata.columns:\n",
    "        raise ValueError(\"metadata is missing required column: 'corpus'\")\n",
    "\n",
    "    corpora = metadata[\"corpus\"].dropna().drop_duplicates().tolist()\n",
    "    metadata_list: list[pd.DataFrame] = []\n",
    "\n",
    "    for corpus in corpora:\n",
    "        if verbose:\n",
    "            print(f\"Current corpus is: {corpus}\")\n",
    "\n",
    "        try:\n",
    "            filtered_metadata = metadata[metadata[\"corpus\"] == corpus]\n",
    "\n",
    "            known_path = base_loc / split / corpus / \"known_raw.jsonl\"\n",
    "            unknown_path = base_loc / split / corpus / \"unknown_raw.jsonl\"\n",
    "\n",
    "            known_data = read_jsonl(str(known_path))\n",
    "            unknown_data = read_jsonl(str(unknown_path))\n",
    "\n",
    "            known_data = apply_temp_doc_id(known_data)\n",
    "            unknown_data = apply_temp_doc_id(unknown_data)\n",
    "\n",
    "            agg_metadata = build_metadata_df(filtered_metadata, known_data, unknown_data)\n",
    "            \n",
    "            # add split as data_type, placed immediately before 'problem'\n",
    "            agg_metadata[\"data_type\"] = split\n",
    "            cols = agg_metadata.columns.tolist()\n",
    "            cols.insert(cols.index(\"problem\"), cols.pop(cols.index(\"data_type\")))\n",
    "            agg_metadata = agg_metadata[cols]\n",
    "            agg_metadata['filename'] = agg_metadata['known_doc_id'] + ' vs ' + agg_metadata['unknown_doc_id'] + '.xlsx'\n",
    "            \n",
    "            metadata_list.append(agg_metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FAILED] corpus='{corpus}' split='{split}': {type(e).__name__}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not metadata_list:\n",
    "        if verbose:\n",
    "            print(\"No corpora succeeded; returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metadata_df = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    if write_file:\n",
    "        print(\"Saving file\")\n",
    "        write_rds(metadata_df, metadata_save_loc)\n",
    "    else:\n",
    "        return metadata_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b817a5a",
   "metadata": {},
   "source": [
    "## Create the Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f70d575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current corpus is: ACL\n",
      "Current corpus is: Amazon\n",
      "Current corpus is: The Apricity\n",
      "Current corpus is: Koppel's Blogs\n",
      "Current corpus is: Enron\n",
      "Current corpus is: Perverted Justice\n",
      "Current corpus is: Reddit\n",
      "Current corpus is: StackExchange\n",
      "Current corpus is: The Telegraph\n",
      "Current corpus is: Yelp\n",
      "Current corpus is: All-the-news\n",
      "[FAILED] corpus='All-the-news' split='training': IndexError: single positional indexer is out-of-bounds\n",
      "Current corpus is: IMDB\n",
      "Current corpus is: TripAdvisor\n",
      "Current corpus is: Wiki\n",
      "Saving file\n"
     ]
    }
   ],
   "source": [
    "build_split_metadata(\n",
    "    base_loc=base_loc,\n",
    "    split= \"training\",\n",
    "    verbose = True,\n",
    "    write_file = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e1cc5",
   "metadata": {},
   "source": [
    "## Create the Test Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca5f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current corpus is: ACL\n",
      "Current corpus is: Amazon\n",
      "Current corpus is: The Apricity\n",
      "Current corpus is: Koppel's Blogs\n",
      "Current corpus is: Enron\n",
      "Current corpus is: Perverted Justice\n",
      "Current corpus is: Reddit\n",
      "Current corpus is: StackExchange\n",
      "Current corpus is: The Telegraph\n",
      "Current corpus is: Yelp\n",
      "Current corpus is: All-the-news\n",
      "Current corpus is: IMDB\n",
      "Current corpus is: TripAdvisor\n",
      "Current corpus is: Wiki\n",
      "Saving file\n"
     ]
    }
   ],
   "source": [
    "build_split_metadata(\n",
    "    base_loc=base_loc,\n",
    "    split= \"test\",\n",
    "    verbose = True,\n",
    "    write_file = True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
