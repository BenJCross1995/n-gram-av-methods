{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aff6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "from from_root import from_root\n",
    "\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from read_and_write_docs import read_excel_sheets, read_rds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c921685",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt2\"]\n",
    "\n",
    "corpuses = [\"Wiki\"]\n",
    "\n",
    "data_types = [\"test\"]\n",
    "\n",
    "base_loc = \"/Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs\"\n",
    "\n",
    "metadata_base_loc = \"/Volumes/BCross/datasets/author_verification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48aa9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_by_corpus_dt = {\n",
    "    (\"Wiki\", \"training\"): 5000,\n",
    "    (\"Wiki\", \"test\"): 672,\n",
    "    (\"Perverted Justice\", \"training\"): 3000,\n",
    "    (\"Perverted Justice\", \"test\"): 574,\n",
    "    (\"Enron\", \"training\"): 5000,\n",
    "    (\"Enron\", \"test\"): 340,\n",
    "    (\"ACL\", \"training\"): 5000,\n",
    "    (\"ACL\", \"test\"): 280,\n",
    "    (\"StackExchange\", \"training\"): 5000,\n",
    "    (\"StackExchange\", \"test\"): 228,\n",
    "    (\"TripAdvisor\", \"training\"): 5000,\n",
    "    (\"TripAdvisor\", \"test\"): 480,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ca3e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_xlsx_files(\n",
    "    directory: Union[str, Path],\n",
    "    *,\n",
    "    recursive: bool = False,\n",
    "    include_temp: bool = False,\n",
    "    sort: bool = True,\n",
    ") -> List[Path]:\n",
    "    \"\"\"\n",
    "    Return all .xlsx files in a directory as a list of Paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str | Path\n",
    "        Directory to search.\n",
    "    recursive : bool\n",
    "        If True, search subdirectories too.\n",
    "    include_temp : bool\n",
    "        If True, include Excel temp files like \"~$something.xlsx\".\n",
    "    sort : bool\n",
    "        If True, sort results by path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Path]\n",
    "        Paths to .xlsx files.\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    if not directory.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "    if not directory.is_dir():\n",
    "        raise NotADirectoryError(f\"Not a directory: {directory}\")\n",
    "\n",
    "    pattern = \"**/*.xlsx\" if recursive else \"*.xlsx\"\n",
    "    files = list(directory.glob(pattern))\n",
    "\n",
    "    if not include_temp:\n",
    "        files = [p for p in files if not p.name.startswith(\"~$\")]\n",
    "\n",
    "    if sort:\n",
    "        files = sorted(files)\n",
    "\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f07aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_counts_to_expected_map(\n",
    "    base_loc,\n",
    "    data_types,\n",
    "    corpuses,\n",
    "    models,\n",
    "    expected_by_corpus_dt,\n",
    "    *,\n",
    "    recursive=False,\n",
    "    raw_prefix=\"raw\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare actual .xlsx counts on disk vs expected counts (expected varies by (corpus, data_type)).\n",
    "\n",
    "    Supports:\n",
    "      {base_loc}/{data_type}/{corpus}/{model}/raw\n",
    "      {base_loc}/{data_type}/{corpus}/{model}/raw_100\n",
    "      {base_loc}/{data_type}/{corpus}/{model}/raw_200\n",
    "      ...\n",
    "\n",
    "    Adds column:\n",
    "      - max_context_length: int suffix after f\"{raw_prefix}_\" or None for f\"{raw_prefix}\"\n",
    "    \"\"\"\n",
    "    base_loc = Path(base_loc)\n",
    "    rows = []\n",
    "\n",
    "    raw_suffix_re = re.compile(rf\"^{re.escape(raw_prefix)}_(\\d+)$\")\n",
    "\n",
    "    def iter_raw_dirs(model_root: Path):\n",
    "        \"\"\"Return list of (data_loc, max_context_length) for raw and raw_### directories.\"\"\"\n",
    "        found = []\n",
    "        if model_root.exists():\n",
    "            for child in model_root.iterdir():\n",
    "                if not child.is_dir():\n",
    "                    continue\n",
    "\n",
    "                if child.name == raw_prefix:\n",
    "                    found.append((child, None))\n",
    "                    continue\n",
    "\n",
    "                m = raw_suffix_re.match(child.name)\n",
    "                if m:\n",
    "                    found.append((child, int(m.group(1))))\n",
    "\n",
    "        # If nothing exists yet, preserve old behaviour by emitting a single raw row (actual will be 0)\n",
    "        if not found:\n",
    "            return [(model_root / raw_prefix, None)]\n",
    "\n",
    "        # Sort None first, then ascending numeric\n",
    "        found.sort(key=lambda x: (-1 if x[1] is None else x[1]))\n",
    "        return found\n",
    "\n",
    "    for data_type in data_types:\n",
    "        for corpus in corpuses:\n",
    "            expected = expected_by_corpus_dt.get((corpus, data_type), None)\n",
    "\n",
    "            for model in models:\n",
    "                model_root = base_loc / data_type / corpus / model\n",
    "\n",
    "                for data_loc, max_context_length in iter_raw_dirs(model_root):\n",
    "                    if data_loc.exists():\n",
    "                        actual = len(list_xlsx_files(data_loc, recursive=recursive))\n",
    "                    else:\n",
    "                        actual = 0\n",
    "\n",
    "                    delta = (actual - expected) if expected is not None else None\n",
    "\n",
    "                    rows.append({\n",
    "                        \"data_type\": data_type,\n",
    "                        \"corpus\": corpus,\n",
    "                        \"model\": model,\n",
    "                        \"max_context_length\": max_context_length,\n",
    "                        \"expected_num_files\": expected,\n",
    "                        \"actual_num_files\": actual,\n",
    "                        \"delta\": delta,\n",
    "                        \"missing\": (expected - actual) if expected is not None and actual < expected else 0 if expected is not None else None,\n",
    "                        \"extra\": (actual - expected) if expected is not None and actual > expected else 0 if expected is not None else None,\n",
    "                        \"status\": (\n",
    "                            \"NOT_STARTED\" if expected is not None and actual == 0\n",
    "                            else \"COMPLETED\" if expected is not None and actual == expected\n",
    "                            else \"MISSING\" if expected is not None and actual < expected\n",
    "                            else \"EXTRA\" if expected is not None and actual > expected\n",
    "                            else \"NO_EXPECTATION\"\n",
    "                        ),\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df[\"_mcl_sort\"] = df[\"max_context_length\"].fillna(-1).astype(int)\n",
    "        df = (\n",
    "            df.sort_values([\"data_type\", \"corpus\", \"model\", \"_mcl_sort\"])\n",
    "              .drop(columns=[\"_mcl_sort\"])\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    # Handy lookup now needs max_context_length in the key (otherwise raw/raw_100 collide)\n",
    "    lookup = {\n",
    "        (r.data_type, r.corpus, r.model, r.max_context_length): r._asdict()\n",
    "        for r in df.itertuples(index=False)\n",
    "    }\n",
    "\n",
    "    return df, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b3c2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df, actual_lookup = compare_counts_to_expected_map(base_loc, data_types, corpuses, models, expected_by_corpus_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36172cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>corpus</th>\n",
       "      <th>model</th>\n",
       "      <th>max_context_length</th>\n",
       "      <th>expected_num_files</th>\n",
       "      <th>actual_num_files</th>\n",
       "      <th>delta</th>\n",
       "      <th>missing</th>\n",
       "      <th>extra</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>200.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>400.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>600.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>700.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>800.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>900.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_type corpus model  max_context_length  expected_num_files  \\\n",
       "0       test   Wiki  gpt2                 NaN                 672   \n",
       "1       test   Wiki  gpt2               100.0                 672   \n",
       "2       test   Wiki  gpt2               200.0                 672   \n",
       "3       test   Wiki  gpt2               300.0                 672   \n",
       "4       test   Wiki  gpt2               400.0                 672   \n",
       "5       test   Wiki  gpt2               500.0                 672   \n",
       "6       test   Wiki  gpt2               600.0                 672   \n",
       "7       test   Wiki  gpt2               700.0                 672   \n",
       "8       test   Wiki  gpt2               800.0                 672   \n",
       "9       test   Wiki  gpt2               900.0                 672   \n",
       "10      test   Wiki  gpt2              1000.0                 672   \n",
       "\n",
       "    actual_num_files  delta  missing  extra     status  \n",
       "0                672      0        0      0  COMPLETED  \n",
       "1                672      0        0      0  COMPLETED  \n",
       "2                672      0        0      0  COMPLETED  \n",
       "3                672      0        0      0  COMPLETED  \n",
       "4                672      0        0      0  COMPLETED  \n",
       "5                672      0        0      0  COMPLETED  \n",
       "6                672      0        0      0  COMPLETED  \n",
       "7                672      0        0      0  COMPLETED  \n",
       "8                672      0        0      0  COMPLETED  \n",
       "9                672      0        0      0  COMPLETED  \n",
       "10               672      0        0      0  COMPLETED  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33fd64c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>corpus</th>\n",
       "      <th>model</th>\n",
       "      <th>max_context_length</th>\n",
       "      <th>expected_num_files</th>\n",
       "      <th>actual_num_files</th>\n",
       "      <th>delta</th>\n",
       "      <th>missing</th>\n",
       "      <th>extra</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>200.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>400.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>600.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>700.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>800.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>900.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_type corpus model  max_context_length  expected_num_files  \\\n",
       "0       test   Wiki  gpt2                 NaN                 672   \n",
       "1       test   Wiki  gpt2               100.0                 672   \n",
       "2       test   Wiki  gpt2               200.0                 672   \n",
       "3       test   Wiki  gpt2               300.0                 672   \n",
       "4       test   Wiki  gpt2               400.0                 672   \n",
       "5       test   Wiki  gpt2               500.0                 672   \n",
       "6       test   Wiki  gpt2               600.0                 672   \n",
       "7       test   Wiki  gpt2               700.0                 672   \n",
       "8       test   Wiki  gpt2               800.0                 672   \n",
       "9       test   Wiki  gpt2               900.0                 672   \n",
       "10      test   Wiki  gpt2              1000.0                 672   \n",
       "\n",
       "    actual_num_files  delta  missing  extra     status  \n",
       "0                672      0        0      0  COMPLETED  \n",
       "1                672      0        0      0  COMPLETED  \n",
       "2                672      0        0      0  COMPLETED  \n",
       "3                672      0        0      0  COMPLETED  \n",
       "4                672      0        0      0  COMPLETED  \n",
       "5                672      0        0      0  COMPLETED  \n",
       "6                672      0        0      0  COMPLETED  \n",
       "7                672      0        0      0  COMPLETED  \n",
       "8                672      0        0      0  COMPLETED  \n",
       "9                672      0        0      0  COMPLETED  \n",
       "10               672      0        0      0  COMPLETED  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "started_data = actual_df[actual_df['status'] != 'NOT_STARTED']\n",
    "started_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "472d2fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>corpus</th>\n",
       "      <th>model</th>\n",
       "      <th>max_context_length</th>\n",
       "      <th>expected_num_files</th>\n",
       "      <th>actual_num_files</th>\n",
       "      <th>delta</th>\n",
       "      <th>missing</th>\n",
       "      <th>extra</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>200.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>400.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>600.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>700.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>800.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>900.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_type corpus model  max_context_length  expected_num_files  \\\n",
       "0       test   Wiki  gpt2                 NaN                 672   \n",
       "1       test   Wiki  gpt2               100.0                 672   \n",
       "2       test   Wiki  gpt2               200.0                 672   \n",
       "3       test   Wiki  gpt2               300.0                 672   \n",
       "4       test   Wiki  gpt2               400.0                 672   \n",
       "5       test   Wiki  gpt2               500.0                 672   \n",
       "6       test   Wiki  gpt2               600.0                 672   \n",
       "7       test   Wiki  gpt2               700.0                 672   \n",
       "8       test   Wiki  gpt2               800.0                 672   \n",
       "9       test   Wiki  gpt2               900.0                 672   \n",
       "10      test   Wiki  gpt2              1000.0                 672   \n",
       "\n",
       "    actual_num_files  delta  missing  extra     status  \n",
       "0                672      0        0      0  COMPLETED  \n",
       "1                672      0        0      0  COMPLETED  \n",
       "2                672      0        0      0  COMPLETED  \n",
       "3                672      0        0      0  COMPLETED  \n",
       "4                672      0        0      0  COMPLETED  \n",
       "5                672      0        0      0  COMPLETED  \n",
       "6                672      0        0      0  COMPLETED  \n",
       "7                672      0        0      0  COMPLETED  \n",
       "8                672      0        0      0  COMPLETED  \n",
       "9                672      0        0      0  COMPLETED  \n",
       "10               672      0        0      0  COMPLETED  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_data = actual_df[actual_df['status'] == 'COMPLETED']\n",
    "completed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cb95d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_complete_to_metadata(metadata_base_loc, data_type, corpus, model, excel_files):\n",
    "    \n",
    "    metadata_loc = f\"{metadata_base_loc}/{data_type}/doc_level_metadata.rds\"\n",
    "    \n",
    "    metadata = read_rds(metadata_loc)\n",
    "    metadata = metadata[metadata['corpus'] == corpus]\n",
    "    metadata['scoring_model'] = model\n",
    "    \n",
    "    file_names = [ef.name for ef in excel_files]\n",
    "    # df with filename + completed=True\n",
    "    df = pd.DataFrame({\n",
    "        \"filename\": file_names,\n",
    "        \"completed\": True\n",
    "    })\n",
    "\n",
    "    # left join onto metadata_df and fill missing completed with False\n",
    "    metadata_df = (\n",
    "        metadata\n",
    "        .merge(df, on=\"filename\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    metadata_df[\"completed\"] = metadata_df[\"completed\"].fillna(False).astype(bool)\n",
    "    metadata_df[\"scored\"] = False\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3317d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_problem_complete_metadata(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups by (data_type, corpus, scoring_model, problem) and returns:\n",
    "      - num_files: total rows\n",
    "      - files_completed: count where completed == True\n",
    "      - files_scored: count where scored == True\n",
    "      - problem_completed: True if num_files == files_scored\n",
    "    \"\"\"\n",
    "    group_cols = [\"data_type\", \"corpus\", \"scoring_model\", \"problem\"]\n",
    "\n",
    "    out = (\n",
    "        metadata\n",
    "        .groupby(group_cols, dropna=False)\n",
    "        .agg(\n",
    "            num_files=(\"filename\", \"size\"),\n",
    "            files_completed=(\"completed\", lambda s: int(s.fillna(False).astype(bool).sum())),\n",
    "            files_scored=(\"scored\", lambda s: int(s.fillna(False).astype(bool).sum())),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    out[\"problem_completed\"] = out[\"num_files\"] == out[\"files_scored\"]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f9555c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_token_level_raw_scores(\n",
    "    completed_df: pd.DataFrame,\n",
    "    base_loc: str | Path,\n",
    "    metadata_base_loc: str | Path,\n",
    "    *,\n",
    "    sheet_name: str = \"metadata\",\n",
    "    output_name: str = \"token_level_raw_scores.xlsx\",\n",
    "    recursive: bool = False,\n",
    "    engine: str | None = None,\n",
    "    overwrite: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For each row in completed_df, read all .xlsx files in:\n",
    "        {base_loc}/{data_type}/{corpus}/{model}/raw\n",
    "    Extract `sheet_name`, concat, sort by sample_id then min_token_size,\n",
    "    and save to:\n",
    "        {base_loc}/{data_type}/{corpus}/{model}/{output_name}\n",
    "\n",
    "    Skips if output file already exists.\n",
    "    \"\"\"\n",
    "    base_loc = Path(base_loc)\n",
    "\n",
    "    required_cols = {\"data_type\", \"corpus\", \"model\"}\n",
    "    missing = required_cols - set(completed_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"completed_df is missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    for row in completed_df.itertuples(index=False):\n",
    "        data_type = getattr(row, \"data_type\")\n",
    "        corpus = getattr(row, \"corpus\")\n",
    "        model = getattr(row, \"model\")\n",
    "\n",
    "        raw_dir = base_loc / data_type / corpus / model / \"raw\"\n",
    "        out_path = raw_dir.parent / output_name  # removes /raw\n",
    "        metadata_out_path = base_loc / data_type / corpus / model / \"raw_problem_metadata.xlsx\"\n",
    "        summary_metadata_out_path = base_loc / data_type / corpus / model / \"raw_problem_completed_metadata.xlsx\"\n",
    "        \n",
    "        # skip if output already exists\n",
    "        if not overwrite:\n",
    "            if out_path.exists():\n",
    "                print(f\"SKIP (exists): {out_path}\")\n",
    "                continue\n",
    "\n",
    "        # if raw dir missing / empty, skip\n",
    "        if not raw_dir.exists():\n",
    "            print(f\"SKIP (no dir): {raw_dir}\")\n",
    "            continue\n",
    "\n",
    "        excel_files = list_xlsx_files(raw_dir, recursive=recursive)\n",
    "        if not excel_files:\n",
    "            print(f\"SKIP (no files): {raw_dir}\")\n",
    "            continue\n",
    "\n",
    "        base_metadata = compare_complete_to_metadata(metadata_base_loc, data_type, corpus, model, excel_files)\n",
    "        \n",
    "        combined_metadata = []\n",
    "\n",
    "        for ef in excel_files:\n",
    "            f_name = ef.name\n",
    "            try:\n",
    "                data = read_excel_sheets(ef, [sheet_name])\n",
    "                combined_metadata.append(data[sheet_name])\n",
    "                \n",
    "                # âœ… mark as scored if read succeeded\n",
    "                base_metadata.loc[base_metadata[\"filename\"] == f_name, \"scored\"] = True\n",
    "            except Exception as e:\n",
    "                print(f\"  WARN: failed reading {sheet_name} from {ef}: {e}\")\n",
    "\n",
    "        if not combined_metadata:\n",
    "            print(f\"SKIP (no readable sheets): {raw_dir}\")\n",
    "            continue\n",
    "\n",
    "        results = (\n",
    "            pd.concat(combined_metadata, ignore_index=True)\n",
    "            .sort_values([\"sample_id\", \"min_token_size\"], ascending=[True, True], kind=\"mergesort\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # insert data_type before corpus, scoring_model after corpus\n",
    "        if \"corpus\" in results.columns:\n",
    "            corpus_idx = results.columns.get_loc(\"corpus\")\n",
    "\n",
    "            if \"data_type\" in results.columns:\n",
    "                results.drop(columns=[\"data_type\"], inplace=True)\n",
    "            results.insert(corpus_idx, \"data_type\", data_type)\n",
    "\n",
    "            corpus_idx = results.columns.get_loc(\"corpus\")  # re-fetch\n",
    "            if \"scoring_model\" in results.columns:\n",
    "                results.drop(columns=[\"scoring_model\"], inplace=True)\n",
    "            results.insert(corpus_idx + 1, \"scoring_model\", model)\n",
    "            \n",
    "        # move problem before known_author (always move; adjust index if problem was before)\n",
    "        if \"problem\" in results.columns and \"known_author\" in results.columns:\n",
    "            problem_idx = results.columns.get_loc(\"problem\")\n",
    "            known_author_idx = results.columns.get_loc(\"known_author\")\n",
    "\n",
    "            problem_col = results.pop(\"problem\")\n",
    "\n",
    "            # if problem was before known_author, known_author shifted left by 1 after pop\n",
    "            if problem_idx < known_author_idx:\n",
    "                known_author_idx -= 1\n",
    "\n",
    "            results.insert(known_author_idx, \"problem\", problem_col)\n",
    "        \n",
    "        # ensure parent exists, then save\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        results.to_excel(out_path, index=False)\n",
    "        print(f\"SAVED: {out_path}  (rows={len(results)})\")\n",
    "        \n",
    "        # Also want to save the metadata\n",
    "        base_metadata.to_excel(metadata_out_path, index=False)\n",
    "        \n",
    "        summary_metadata = create_problem_complete_metadata(base_metadata)\n",
    "        summary_metadata.to_excel(summary_metadata_out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9579f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/Qwen2.5-0.5B-Instruct/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/Qwen2.5-1.5B-Instruct/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Enron/gemma-3-270m/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/Qwen2.5-0.5B-Instruct/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/Qwen2.5-1.5B-Instruct/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gemma-3-270m/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/Qwen2.5-0.5B-Instruct/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/Qwen2.5-1.5B-Instruct/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gemma-3-270m/token_level_raw_scores.xlsx\n",
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Wiki/gpt2/token_level_raw_scores.xlsx\n"
     ]
    }
   ],
   "source": [
    "build_and_save_token_level_raw_scores(\n",
    "    completed_data,\n",
    "    base_loc,\n",
    "    metadata_base_loc,\n",
    "    sheet_name = \"metadata\",\n",
    "    output_name = \"token_level_raw_scores.xlsx\",\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f02c112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>corpus</th>\n",
       "      <th>model</th>\n",
       "      <th>expected_num_files</th>\n",
       "      <th>actual_num_files</th>\n",
       "      <th>delta</th>\n",
       "      <th>missing</th>\n",
       "      <th>extra</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test</td>\n",
       "      <td>Perverted Justice</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>574</td>\n",
       "      <td>491</td>\n",
       "      <td>-83</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_type             corpus model  expected_num_files  actual_num_files  \\\n",
       "11      test  Perverted Justice  gpt2                 574               491   \n",
       "\n",
       "    delta  missing  extra   status  \n",
       "11    -83       83      0  MISSING  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_df = actual_df[\n",
    "    (actual_df['data_type'] == 'test')\n",
    "    & (actual_df['corpus'] == 'Perverted Justice')\n",
    "    & (actual_df['model'] == 'gpt2')\n",
    "]\n",
    "\n",
    "manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "53d0767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIP (exists): /Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs/test/Perverted Justice/gpt2/token_level_raw_scores.xlsx\n"
     ]
    }
   ],
   "source": [
    "build_and_save_token_level_raw_scores(\n",
    "    manual_df,\n",
    "    base_loc,\n",
    "    metadata_base_loc,\n",
    "    sheet_name = \"metadata\",\n",
    "    output_name = \"token_level_raw_scores.xlsx\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
