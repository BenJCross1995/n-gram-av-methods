{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d3fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from from_root import from_root\n",
    "\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from model_loading import load_model, distinct_special_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6dcd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"/Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs\"\n",
    "\n",
    "file_name = \"raw_all_tokens.xlsx\"\n",
    "\n",
    "file_loc = f\"{base_loc}/{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a50984",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = \"/Volumes/BCross/models/gpt2\"\n",
    "\n",
    "tokenizer, model = load_model(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf247128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġ', 'ĉ', 'Ċ', 'č', 'ċ']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = distinct_special_chars(tokenizer=tokenizer)\n",
    "special_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2601f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_tokens_are_lists(df):\n",
    "    df = df.copy()\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f180784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_min_length(df, min_tokens: int = 2):\n",
    "    \n",
    "    df = df[df['num_tokens'] >= min_tokens]\n",
    "    \n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2379f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_only_special_tokens(df: pd.DataFrame, special_token_list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    REMOVE rows where df['tokens'] contains ONLY special tokens (and at least 1 token).\n",
    "    Assumes df['tokens'] is a list of token strings per row.\n",
    "    \"\"\"\n",
    "    special = set(special_token_list)\n",
    "    only_special = df[\"tokens\"].apply(lambda toks: bool(toks) and all(t in special for t in toks))\n",
    "    return df.loc[~only_special].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa2ed05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_only_numbers_and_special_tokens(df: pd.DataFrame, special_token_list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    REMOVE rows where df['tokens'] contains ONLY numbers plus any strings in special_token_list.\n",
    "    Example removed: [\"<special>19\", \"<special>97\", \"112\"]  (assuming \"<special>\" is in special_token_list)\n",
    "    Assumes df['tokens'] is a list (items can be str/int).\n",
    "    \"\"\"\n",
    "    special = sorted(set(map(str, special_token_list)), key=len, reverse=True)\n",
    "\n",
    "    def is_only_numbers_plus_special(toks) -> bool:\n",
    "        s = \"\".join(map(str, toks))  # combine tokens into one string\n",
    "        for sp in special:\n",
    "            s = s.replace(sp, \"\")\n",
    "        s = \"\".join(s.split())       # drop any remaining whitespace\n",
    "        return bool(s) and bool(re.fullmatch(r\"\\d+\", s))\n",
    "\n",
    "    mask = df[\"tokens\"].apply(is_only_numbers_plus_special)\n",
    "    return df.loc[~mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94305666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_only_punct_and_special_tokens(df: pd.DataFrame, special_token_list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    REMOVE rows where df['tokens'] contains ONLY punctuation and/or special tokens,\n",
    "    allowing special tokens to appear as substrings inside tokens (e.g. \"<special>,\").\n",
    "    Assumes df['tokens'] is a list (items will be cast to str).\n",
    "    \"\"\"\n",
    "    specials = sorted(set(map(str, special_token_list)), key=len, reverse=True)\n",
    "    special_re = re.compile(\"|\".join(re.escape(s) for s in specials)) if specials else None\n",
    "\n",
    "    def strip_specials(s: str) -> str:\n",
    "        s = str(s)\n",
    "        return special_re.sub(\"\", s) if special_re else s\n",
    "\n",
    "    def is_punct_or_empty_after_strip(tok) -> bool:\n",
    "        rem = strip_specials(tok)\n",
    "        rem = \"\".join(rem.split())  # drop whitespace\n",
    "        if rem == \"\":\n",
    "            return True\n",
    "        return all(unicodedata.category(ch).startswith(\"P\") for ch in rem)\n",
    "\n",
    "    only_punct_or_special = df[\"tokens\"].apply(\n",
    "        lambda toks: bool(toks) and all(is_punct_or_empty_after_strip(t) for t in toks)\n",
    "    )\n",
    "\n",
    "    return df.loc[~only_punct_or_special].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce06699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_at_least_n_minus_1_specials(df: pd.DataFrame, special_token_list, n: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    REMOVE rows where:\n",
    "      - if n is set: len(tokens) == n AND (# special-only tokens) >= n-1\n",
    "      - if n is None: (# special-only tokens) >= len(tokens)-1  (i.e., at most 1 non-special)\n",
    "\n",
    "    Handles specials embedded inside tokens by stripping special substrings first.\n",
    "    Assumes df['tokens'] is a list (items cast to str).\n",
    "    \"\"\"\n",
    "    specials = sorted(set(map(str, special_token_list)), key=len, reverse=True)\n",
    "    special_re = re.compile(\"|\".join(re.escape(s) for s in specials)) if specials else None\n",
    "\n",
    "    def is_special_only(tok) -> bool:\n",
    "        s = str(tok)\n",
    "        if special_re:\n",
    "            s = special_re.sub(\"\", s)\n",
    "        s = \"\".join(s.split())  # drop whitespace\n",
    "        return s == \"\"\n",
    "\n",
    "    def should_remove(toks) -> bool:\n",
    "        if not toks:\n",
    "            return False\n",
    "        L = len(toks)\n",
    "        if n is not None and L != n:\n",
    "            return False\n",
    "        special_count = sum(is_special_only(t) for t in toks)\n",
    "        threshold = (n - 1) if n is not None else (L - 1)\n",
    "        return special_count >= threshold\n",
    "\n",
    "    mask = df[\"tokens\"].apply(should_remove)\n",
    "    return df.loc[~mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef11ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_zero_special_tokens(df: pd.DataFrame, special_token_list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    REMOVE rows where df['tokens'] contains ZERO special tokens.\n",
    "    Treats specials as matching anywhere inside a token (substring match).\n",
    "    Assumes df['tokens'] is a list (items cast to str).\n",
    "    \"\"\"\n",
    "    specials = sorted(set(map(str, special_token_list)), key=len, reverse=True)\n",
    "    special_re = re.compile(\"|\".join(re.escape(s) for s in specials)) if specials else None\n",
    "\n",
    "    def has_any_special(toks) -> bool:\n",
    "        if not special_re:\n",
    "            return False\n",
    "        return any(bool(special_re.search(str(t))) for t in toks)\n",
    "\n",
    "    zero_special = df[\"tokens\"].apply(lambda toks: not has_any_special(toks))\n",
    "    return df.loc[~zero_special].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559d8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_leading_specials_single_word(df: pd.DataFrame, special_token_list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    REMOVE rows where:\n",
    "      1) tokens that contain any special substring appear only at the START of the list\n",
    "         (pattern: True...True, False...False with at least one True and one False)\n",
    "      2) after removing special substrings and stripping whitespace, the concatenation is a single word\n",
    "         (letters with optional ' or - parts, e.g. david's, co-op)\n",
    "\n",
    "    Assumes df['tokens'] is a list (items cast to str).\n",
    "    \"\"\"\n",
    "    specials = sorted(set(map(str, special_token_list)), key=len, reverse=True)\n",
    "    special_re = re.compile(\"|\".join(re.escape(s) for s in specials)) if specials else None\n",
    "    word_re = re.compile(r\"[A-Za-z]+(?:[’'-][A-Za-z]+)*\")\n",
    "\n",
    "    def should_remove(toks) -> bool:\n",
    "        if not toks or not special_re:\n",
    "            return False\n",
    "\n",
    "        toks = list(map(str, toks))\n",
    "        has_special = [bool(special_re.search(t)) for t in toks]\n",
    "\n",
    "        # must be specials only at start: True* then False* (and both parts non-empty)\n",
    "        if not any(has_special) or all(has_special):\n",
    "            return False\n",
    "        seen_non = False\n",
    "        for hs in has_special:\n",
    "            if not hs:\n",
    "                seen_non = True\n",
    "            elif seen_non:\n",
    "                return False  # special appears after a non-special token\n",
    "\n",
    "        s = \"\".join(toks)\n",
    "        s = special_re.sub(\"\", s).strip()\n",
    "        return bool(s) and bool(word_re.fullmatch(s))\n",
    "\n",
    "    mask = df[\"tokens\"].apply(should_remove)\n",
    "    return df.loc[~mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10db8000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token count: 71857\n",
      "After filter_min_length: 71850\n",
      "After filter_only_special_tokens: 71849\n",
      "After filter_only_number_and_special_tokens: 71834\n",
      "After filter_only_punct_and_special_tokens: 71690\n",
      "After filter_at_least_n_minus_1_specials: 70954\n",
      "After filter_zero_special_tokens: 69529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_tokens = pd.read_excel(file_loc)\n",
    "raw_tokens = ensure_tokens_are_lists(raw_tokens)\n",
    "print(\"Original token count:\", len(raw_tokens))\n",
    "\n",
    "filtered_tokens = filter_min_length(raw_tokens, min_tokens=2)\n",
    "print(\"After filter_min_length:\", len(filtered_tokens))\n",
    "\n",
    "filtered_tokens = filter_only_special_tokens(filtered_tokens, special_tokens)\n",
    "print(\"After filter_only_special_tokens:\", len(filtered_tokens))\n",
    "\n",
    "filtered_tokens = filter_only_numbers_and_special_tokens(filtered_tokens, special_tokens)\n",
    "print(\"After filter_only_number_and_special_tokens:\", len(filtered_tokens))\n",
    "\n",
    "filtered_tokens = filter_only_punct_and_special_tokens(filtered_tokens, special_tokens)\n",
    "print(\"After filter_only_punct_and_special_tokens:\", len(filtered_tokens))\n",
    "\n",
    "filtered_tokens = filter_at_least_n_minus_1_specials(filtered_tokens, special_tokens)\n",
    "print(\"After filter_at_least_n_minus_1_specials:\", len(filtered_tokens))\n",
    "\n",
    "filtered_tokens = filter_zero_special_tokens(filtered_tokens, special_tokens)\n",
    "print(\"After filter_zero_special_tokens:\", len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75b10f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28740</th>\n",
       "      <td>big brother</td>\n",
       "      <td>[Ġbig, Ġbrother]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59326</th>\n",
       "      <td>. my fianc</td>\n",
       "      <td>[., Ġmy, Ġfian, c]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14811</th>\n",
       "      <td>surnames</td>\n",
       "      <td>[Ġsurn, ames]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6426</th>\n",
       "      <td>locke</td>\n",
       "      <td>[Ġloc, ke]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21168</th>\n",
       "      <td>spaceland</td>\n",
       "      <td>[Ġspac, eland]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>. let</td>\n",
       "      <td>[., Ġlet]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43518</th>\n",
       "      <td>the sand,</td>\n",
       "      <td>[Ġthe, Ġsand, ,]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63586</th>\n",
       "      <td>he is going to</td>\n",
       "      <td>[Ġhe, Ġis, Ġgoing, Ġto]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34192</th>\n",
       "      <td>up\\ni</td>\n",
       "      <td>[Ġup, Ċ, i]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10863</th>\n",
       "      <td>houston</td>\n",
       "      <td>[Ġh, ouston]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18427</th>\n",
       "      <td>have got</td>\n",
       "      <td>[Ġhave, Ġgot]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47968</th>\n",
       "      <td>i'm working</td>\n",
       "      <td>[Ġi, 'm, Ġworking]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12261</th>\n",
       "      <td>bucks..</td>\n",
       "      <td>[Ġbucks, ..]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18568</th>\n",
       "      <td>are this</td>\n",
       "      <td>[Ġare, Ġthis]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34057</th>\n",
       "      <td>'coz</td>\n",
       "      <td>[Ġ', co, z]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60438</th>\n",
       "      <td>go\\nwill you</td>\n",
       "      <td>[Ġgo, Ċ, will, Ġyou]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45838</th>\n",
       "      <td>that i got</td>\n",
       "      <td>[Ġthat, Ġi, Ġgot]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19598</th>\n",
       "      <td>naked and</td>\n",
       "      <td>[Ġnaked, Ġand]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41676</th>\n",
       "      <td>the irish</td>\n",
       "      <td>[Ġthe, Ġir, ish]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19809</th>\n",
       "      <td>learn the</td>\n",
       "      <td>[Ġlearn, Ġthe]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>mom.</td>\n",
       "      <td>[Ġmom, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34311</th>\n",
       "      <td>fb's</td>\n",
       "      <td>[Ġf, b, 's]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61089</th>\n",
       "      <td>of a lot of</td>\n",
       "      <td>[Ġof, Ġa, Ġlot, Ġof]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65127</th>\n",
       "      <td>it would be nice</td>\n",
       "      <td>[Ġit, Ġwould, Ġbe, Ġnice]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8853</th>\n",
       "      <td>writin</td>\n",
       "      <td>[Ġwrit, in]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12797</th>\n",
       "      <td>she and</td>\n",
       "      <td>[Ġshe, Ġand]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39267</th>\n",
       "      <td>'s a good</td>\n",
       "      <td>['s, Ġa, Ġgood]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12224</th>\n",
       "      <td>came up</td>\n",
       "      <td>[Ġcame, Ġup]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60169</th>\n",
       "      <td>. you can t</td>\n",
       "      <td>[., Ġyou, Ġcan, Ġt]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12066</th>\n",
       "      <td>be from</td>\n",
       "      <td>[Ġbe, Ġfrom]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44472</th>\n",
       "      <td>at the end</td>\n",
       "      <td>[Ġat, Ġthe, Ġend]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45969</th>\n",
       "      <td>since it's</td>\n",
       "      <td>[Ġsince, Ġit, 's]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49202</th>\n",
       "      <td>of a former</td>\n",
       "      <td>[Ġof, Ġa, Ġformer]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67495</th>\n",
       "      <td>jasmin and i</td>\n",
       "      <td>[Ġj, as, min, Ġand, Ġi]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19875</th>\n",
       "      <td>to choose</td>\n",
       "      <td>[Ġto, Ġchoose]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>hit up</td>\n",
       "      <td>[Ġhit, Ġup]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12731</th>\n",
       "      <td>to gain</td>\n",
       "      <td>[Ġto, Ġgain]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26176</th>\n",
       "      <td>soon after</td>\n",
       "      <td>[Ġsoon, Ġafter]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55460</th>\n",
       "      <td>every other day</td>\n",
       "      <td>[Ġevery, Ġother, Ġday]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49197</th>\n",
       "      <td>-part series</td>\n",
       "      <td>[-, part, Ġseries]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52263</th>\n",
       "      <td>because i can</td>\n",
       "      <td>[Ġbecause, Ġi, Ġcan]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35585</th>\n",
       "      <td>a low-</td>\n",
       "      <td>[Ġa, Ġlow, -]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71186</th>\n",
       "      <td>i wanted to touch base with you</td>\n",
       "      <td>[Ġi, Ġwanted, Ġto, Ġtouch, Ġbase, Ġwith, Ġyou]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25152</th>\n",
       "      <td>generalize</td>\n",
       "      <td>[Ġgeneral, ize]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>\" to</td>\n",
       "      <td>[\", Ġto]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24603</th>\n",
       "      <td>bodies and</td>\n",
       "      <td>[Ġbodies, Ġand]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>and hit</td>\n",
       "      <td>[Ġand, Ġhit]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62096</th>\n",
       "      <td>the chinese.</td>\n",
       "      <td>[Ġthe, Ġch, inese, .]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21229</th>\n",
       "      <td>he killed</td>\n",
       "      <td>[Ġhe, Ġkilled]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>to our</td>\n",
       "      <td>[Ġto, Ġour]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 phrase  \\\n",
       "28740                       big brother   \n",
       "59326                        . my fianc   \n",
       "14811                          surnames   \n",
       "6426                              locke   \n",
       "21168                         spaceland   \n",
       "2460                              . let   \n",
       "43518                         the sand,   \n",
       "63586                    he is going to   \n",
       "34192                             up\\ni   \n",
       "10863                           houston   \n",
       "18427                          have got   \n",
       "47968                       i'm working   \n",
       "12261                           bucks..   \n",
       "18568                          are this   \n",
       "34057                              'coz   \n",
       "60438                      go\\nwill you   \n",
       "45838                        that i got   \n",
       "19598                         naked and   \n",
       "41676                         the irish   \n",
       "19809                         learn the   \n",
       "3099                               mom.   \n",
       "34311                              fb's   \n",
       "61089                       of a lot of   \n",
       "65127                  it would be nice   \n",
       "8853                             writin   \n",
       "12797                           she and   \n",
       "39267                         's a good   \n",
       "12224                           came up   \n",
       "60169                       . you can t   \n",
       "12066                           be from   \n",
       "44472                        at the end   \n",
       "45969                        since it's   \n",
       "49202                       of a former   \n",
       "67495                      jasmin and i   \n",
       "19875                         to choose   \n",
       "10006                            hit up   \n",
       "12731                           to gain   \n",
       "26176                        soon after   \n",
       "55460                   every other day   \n",
       "49197                      -part series   \n",
       "52263                     because i can   \n",
       "35585                            a low-   \n",
       "71186   i wanted to touch base with you   \n",
       "25152                        generalize   \n",
       "1372                               \" to   \n",
       "24603                        bodies and   \n",
       "14313                           and hit   \n",
       "62096                      the chinese.   \n",
       "21229                         he killed   \n",
       "7390                             to our   \n",
       "\n",
       "                                               tokens  num_tokens  \n",
       "28740                                [Ġbig, Ġbrother]           2  \n",
       "59326                              [., Ġmy, Ġfian, c]           4  \n",
       "14811                                   [Ġsurn, ames]           2  \n",
       "6426                                       [Ġloc, ke]           2  \n",
       "21168                                  [Ġspac, eland]           2  \n",
       "2460                                        [., Ġlet]           2  \n",
       "43518                                [Ġthe, Ġsand, ,]           3  \n",
       "63586                         [Ġhe, Ġis, Ġgoing, Ġto]           4  \n",
       "34192                                     [Ġup, Ċ, i]           3  \n",
       "10863                                    [Ġh, ouston]           2  \n",
       "18427                                   [Ġhave, Ġgot]           2  \n",
       "47968                              [Ġi, 'm, Ġworking]           3  \n",
       "12261                                    [Ġbucks, ..]           2  \n",
       "18568                                   [Ġare, Ġthis]           2  \n",
       "34057                                     [Ġ', co, z]           3  \n",
       "60438                            [Ġgo, Ċ, will, Ġyou]           4  \n",
       "45838                               [Ġthat, Ġi, Ġgot]           3  \n",
       "19598                                  [Ġnaked, Ġand]           2  \n",
       "41676                                [Ġthe, Ġir, ish]           3  \n",
       "19809                                  [Ġlearn, Ġthe]           2  \n",
       "3099                                        [Ġmom, .]           2  \n",
       "34311                                     [Ġf, b, 's]           3  \n",
       "61089                            [Ġof, Ġa, Ġlot, Ġof]           4  \n",
       "65127                       [Ġit, Ġwould, Ġbe, Ġnice]           4  \n",
       "8853                                      [Ġwrit, in]           2  \n",
       "12797                                    [Ġshe, Ġand]           2  \n",
       "39267                                 ['s, Ġa, Ġgood]           3  \n",
       "12224                                    [Ġcame, Ġup]           2  \n",
       "60169                             [., Ġyou, Ġcan, Ġt]           4  \n",
       "12066                                    [Ġbe, Ġfrom]           2  \n",
       "44472                               [Ġat, Ġthe, Ġend]           3  \n",
       "45969                               [Ġsince, Ġit, 's]           3  \n",
       "49202                              [Ġof, Ġa, Ġformer]           3  \n",
       "67495                         [Ġj, as, min, Ġand, Ġi]           5  \n",
       "19875                                  [Ġto, Ġchoose]           2  \n",
       "10006                                     [Ġhit, Ġup]           2  \n",
       "12731                                    [Ġto, Ġgain]           2  \n",
       "26176                                 [Ġsoon, Ġafter]           2  \n",
       "55460                          [Ġevery, Ġother, Ġday]           3  \n",
       "49197                              [-, part, Ġseries]           3  \n",
       "52263                            [Ġbecause, Ġi, Ġcan]           3  \n",
       "35585                                   [Ġa, Ġlow, -]           3  \n",
       "71186  [Ġi, Ġwanted, Ġto, Ġtouch, Ġbase, Ġwith, Ġyou]           7  \n",
       "25152                                 [Ġgeneral, ize]           2  \n",
       "1372                                         [\", Ġto]           2  \n",
       "24603                                 [Ġbodies, Ġand]           2  \n",
       "14313                                    [Ġand, Ġhit]           2  \n",
       "62096                           [Ġthe, Ġch, inese, .]           4  \n",
       "21229                                  [Ġhe, Ġkilled]           2  \n",
       "7390                                      [Ġto, Ġour]           2  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5ff28a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>, i</td>\n",
       "      <td>[,, Ġi]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>nw</td>\n",
       "      <td>[Ġn, w]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>, a</td>\n",
       "      <td>[,, Ġa]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>wp</td>\n",
       "      <td>[Ġw, p]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>uk</td>\n",
       "      <td>[Ġu, k]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>ae</td>\n",
       "      <td>[Ġa, e]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>gt</td>\n",
       "      <td>[Ġg, t]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>dp</td>\n",
       "      <td>[Ġd, p]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1-</td>\n",
       "      <td>[Ġ1, -]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>. g</td>\n",
       "      <td>[., Ġg]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>? i</td>\n",
       "      <td>[?, Ġi]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1,</td>\n",
       "      <td>[Ġ1, ,]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>. i</td>\n",
       "      <td>[., Ġi]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>, j</td>\n",
       "      <td>[,, Ġj]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>, d</td>\n",
       "      <td>[,, Ġd]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1.</td>\n",
       "      <td>[Ġ1, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>. a</td>\n",
       "      <td>[., Ġa]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>. d</td>\n",
       "      <td>[., Ġd]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>. k</td>\n",
       "      <td>[., Ġk]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>w/</td>\n",
       "      <td>[Ġw, /]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>3.</td>\n",
       "      <td>[Ġ3, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2.</td>\n",
       "      <td>[Ġ2, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>. m</td>\n",
       "      <td>[., Ġm]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>\"i</td>\n",
       "      <td>[Ġ\", i]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>. j</td>\n",
       "      <td>[., Ġj]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>! i</td>\n",
       "      <td>[!, Ġi]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>5,</td>\n",
       "      <td>[Ġ5, ,]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>. 2</td>\n",
       "      <td>[., Ġ2]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>6.</td>\n",
       "      <td>[Ġ6, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>. b</td>\n",
       "      <td>[., Ġb]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>e-</td>\n",
       "      <td>[Ġe, -]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>u.</td>\n",
       "      <td>[Ġu, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>bf</td>\n",
       "      <td>[Ġb, f]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>, u</td>\n",
       "      <td>[,, Ġu]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>eu</td>\n",
       "      <td>[Ġe, u]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>b4</td>\n",
       "      <td>[Ġb, 4]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>kc</td>\n",
       "      <td>[Ġk, c]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>gf</td>\n",
       "      <td>[Ġg, f]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>u,</td>\n",
       "      <td>[Ġu, ,]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>jr</td>\n",
       "      <td>[Ġj, r]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>o i</td>\n",
       "      <td>[o, Ġi]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>i w</td>\n",
       "      <td>[i, Ġw]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>u?</td>\n",
       "      <td>[Ġu, ?]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>yu</td>\n",
       "      <td>[Ġy, u]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>. u</td>\n",
       "      <td>[., Ġu]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>2)</td>\n",
       "      <td>[Ġ2, )]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>a,</td>\n",
       "      <td>[Ġa, ,]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>(a</td>\n",
       "      <td>[Ġ(, a]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>i.</td>\n",
       "      <td>[Ġi, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>k-</td>\n",
       "      <td>[Ġk, -]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    phrase   tokens  num_tokens\n",
       "218    , i  [,, Ġi]           2\n",
       "222     nw  [Ġn, w]           2\n",
       "226    , a  [,, Ġa]           2\n",
       "229     wp  [Ġw, p]           2\n",
       "236     uk  [Ġu, k]           2\n",
       "242     ae  [Ġa, e]           2\n",
       "244     gt  [Ġg, t]           2\n",
       "247     dp  [Ġd, p]           2\n",
       "250     1-  [Ġ1, -]           2\n",
       "254    . g  [., Ġg]           2\n",
       "255    ? i  [?, Ġi]           2\n",
       "256     1,  [Ġ1, ,]           2\n",
       "257    . i  [., Ġi]           2\n",
       "258    , j  [,, Ġj]           2\n",
       "259    , d  [,, Ġd]           2\n",
       "260     1.  [Ġ1, .]           2\n",
       "262    . a  [., Ġa]           2\n",
       "264    . d  [., Ġd]           2\n",
       "265    . k  [., Ġk]           2\n",
       "267     w/  [Ġw, /]           2\n",
       "268     3.  [Ġ3, .]           2\n",
       "269     2.  [Ġ2, .]           2\n",
       "273    . m  [., Ġm]           2\n",
       "274     \"i  [Ġ\", i]           2\n",
       "276    . j  [., Ġj]           2\n",
       "277    ! i  [!, Ġi]           2\n",
       "278     5,  [Ġ5, ,]           2\n",
       "279    . 2  [., Ġ2]           2\n",
       "280     6.  [Ġ6, .]           2\n",
       "282    . b  [., Ġb]           2\n",
       "284     e-  [Ġe, -]           2\n",
       "285     u.  [Ġu, .]           2\n",
       "298     bf  [Ġb, f]           2\n",
       "307    , u  [,, Ġu]           2\n",
       "310     eu  [Ġe, u]           2\n",
       "311     b4  [Ġb, 4]           2\n",
       "320     kc  [Ġk, c]           2\n",
       "327     gf  [Ġg, f]           2\n",
       "329     u,  [Ġu, ,]           2\n",
       "339     jr  [Ġj, r]           2\n",
       "343    o i  [o, Ġi]           2\n",
       "344    i w  [i, Ġw]           2\n",
       "346     u?  [Ġu, ?]           2\n",
       "347     yu  [Ġy, u]           2\n",
       "353    . u  [., Ġu]           2\n",
       "370     2)  [Ġ2, )]           2\n",
       "371     a,  [Ġa, ,]           2\n",
       "372     (a  [Ġ(, a]           2\n",
       "373     i.  [Ġi, .]           2\n",
       "374     k-  [Ġk, -]           2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens.head(50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
