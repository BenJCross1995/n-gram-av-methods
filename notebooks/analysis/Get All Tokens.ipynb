{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e041564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from from_root import from_root\n",
    "\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from utils import list_xlsx_files\n",
    "from read_and_write_docs import read_excel_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c68a51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt2\"]\n",
    "\n",
    "corpora = [\"Wiki\", \"Enron\", \"Perverted Justice\", \"StackExchange\", \"ACL\", \"TripAdvisor\", \"The Apricity\", \"Koppel's Blogs\"]\n",
    "\n",
    "data_types = [\"training\", \"test\"]\n",
    "\n",
    "base_loc = \"/Volumes/BCross/av_datasets_experiments/ngram_masking_logrpobs\"\n",
    "\n",
    "metadata_base_loc = \"/Volumes/BCross/datasets/author_verification\"\n",
    "\n",
    "save_file_name = \"raw_all_tokens.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d88085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wokring on training - Wiki - gpt2\n",
      "Wokring on training - Enron - gpt2\n",
      "Wokring on training - Perverted Justice - gpt2\n",
      "Wokring on training - StackExchange - gpt2\n",
      "Wokring on training - ACL - gpt2\n",
      "Wokring on training - TripAdvisor - gpt2\n",
      "Wokring on training - The Apricity - gpt2\n",
      "Wokring on training - Koppel's Blogs - gpt2\n",
      "Wokring on test - Wiki - gpt2\n",
      "Wokring on test - Enron - gpt2\n",
      "Wokring on test - Perverted Justice - gpt2\n",
      "Wokring on test - StackExchange - gpt2\n",
      "Wokring on test - ACL - gpt2\n",
      "Wokring on test - TripAdvisor - gpt2\n",
      "Wokring on test - The Apricity - gpt2\n",
      "Wokring on test - Koppel's Blogs - gpt2\n"
     ]
    }
   ],
   "source": [
    "base_loc = Path(base_loc)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for data_type in data_types:\n",
    "    for corpus in corpora:\n",
    "        for model in models:\n",
    "            print(f\"Wokring on {data_type} - {corpus} - {model}\")\n",
    "\n",
    "            file_dir = base_loc / data_type / corpus / model / \"raw_100\"\n",
    "            if not file_dir.exists():\n",
    "                continue\n",
    "\n",
    "            excel_files = list_xlsx_files(file_dir)\n",
    "            for f in excel_files:\n",
    "                file_loc = file_dir / f\n",
    "\n",
    "                try:\n",
    "                    data = read_excel_sheets(file_loc, sheet_names=[\"phrase score\"])\n",
    "                    phrase_scores = data[\"phrase score\"]\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # keep only what you want\n",
    "                distinct_phrases = phrase_scores[[\"phrase\", \"tokens\", \"num_tokens\"]]\n",
    "                tokens.append(distinct_phrases)\n",
    "\n",
    "# concat once, then drop duplicates once at the end\n",
    "if tokens:\n",
    "    full_tokens = pd.concat(tokens, ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "else:\n",
    "    full_tokens = pd.DataFrame(columns=[\"phrase\", \"tokens\", \"num_tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53a04332",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokens = full_tokens.copy()\n",
    "\n",
    "full_tokens[\"phrase_len\"] = full_tokens[\"phrase\"].astype(str).str.len()\n",
    "\n",
    "full_tokens = (\n",
    "    full_tokens\n",
    "    .sort_values([\"num_tokens\", \"phrase_len\"], ascending=[True, True])\n",
    "    .drop(columns=[\"phrase_len\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f7d8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = base_loc / save_file_name\n",
    "full_tokens.to_excel(save_loc, index=False, engine=\"openpyxl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
